{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network - Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_total_data_nn(datas):\n",
    "    data_sim_sign = datas[1].sample(frac=1).reset_index(drop=True)\n",
    "    data_bkg = datas[0].sample(frac=1).reset_index(drop=True)\n",
    "    data_sig_train = data_sim_sign\n",
    "    data_bkg_train = data_bkg[:len(data_sig_train)]\n",
    "    data_train = [data_sig_train, data_bkg_train]\n",
    "#    data_sig_test = data_sim_sign[int(train_size*len(data_sim_sign)):]\n",
    "#    data_bkg_test = data_bkg[int(train_size*len(data_bkg)):]\n",
    "#    data_test = data_sig_test.append(data_bkg_test[:len(data_sig_test)],\n",
    "#                                     ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "#    x_test = data_test.iloc[:,:-1]\n",
    "#    y_test = data_test.iloc[:,-1]\n",
    "#    x_test.to_csv('Data_test/x_test_nn_ensemble.csv', mode='w+')\n",
    "#    y_test.to_csv('Data_test/y_test_nn_ensemble.csv', mode='w+')\n",
    "    return data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(x_train, y_train):\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    list_k = list(skf.split(x_train_nn, y_train_nn))\n",
    "    return list_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_data(x_train, y_train, fold):\n",
    "    X_train = x_train[fold[0]]\n",
    "    Y_train = y_train[fold[0]]\n",
    "    X_val = x_train[fold[1]]\n",
    "    Y_val = y_train[fold[1]]\n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nn_evolution(tr_acc, tr_loss, val_acc, val_loss):\n",
    "    epochs = range(1, len(tr_acc)+1)\n",
    "    fig = plt.figure(figsize=(20,7))\n",
    "    \n",
    "    ax_acc = plt.subplot(1,2,1)\n",
    "    ax_acc.plot(epochs, tr_acc, 'ro', label='Training accuracy')\n",
    "    ax_acc.plot(epochs, val_acc, 'g', label='Validation accuracy')\n",
    "    ax_acc.set_title('Training and validation accuracy')\n",
    "    ax_acc.set_xlabel('Epochs')\n",
    "    ax_acc.set_ylabel('Accuracy')\n",
    "    ax_acc.legend()\n",
    "    \n",
    "    ax_loss = plt.subplot(1,2,2)\n",
    "    ax_loss.plot(epochs, tr_loss, marker='o', color='orange', label='Training loss')\n",
    "    ax_loss.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    ax_loss.set_title('Training and validation loss')\n",
    "    ax_loss.set_xlabel('Epochs')\n",
    "    ax_loss.set_ylabel('Loss')\n",
    "    ax_loss.legend()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network - Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, layers_size=[30,30], batch_normalization=True, dropout=None):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(layers_size[0], activation='relu', input_shape=(input_shape,)))\n",
    "    if batch_normalization:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    for layer_size in layers_size[1:]:\n",
    "        model.add(layers.Dense(layer_size, activation='relu'))\n",
    "        if dropout!=None:\n",
    "            model.add(layers.Dropout(dropout))\n",
    "        if batch_normalization:\n",
    "            model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(x_train, y_train, epochs=50, layers_size=[30,30], batch_normalization=True,\n",
    "                dropout=None, batch_size=10, verbose=0, return_max_val_acc=False, show_graph=True):\n",
    "    k_folds_indx = train_val_split(x_train_nn, y_train_nn)\n",
    "\n",
    "    tr_acc, tr_loss, val_acc, val_loss = [], [], [], []\n",
    "    \n",
    "    for i,fold in enumerate(k_folds_indx):\n",
    "        print('Processing', i+1, '/', len(k_folds_indx), 'fold.')\n",
    "        X_train, Y_train, X_val, Y_val = get_kfold_data(x_train_nn, y_train_nn, fold)\n",
    "        model = build_model(input_shape=x_train.shape[1],\n",
    "                            layers_size=layers_size,\n",
    "                            batch_normalization=batch_normalization,\n",
    "                           dropout=dropout)\n",
    "        history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
    "                            validation_data=(X_val, Y_val), verbose=verbose)\n",
    "        history_dict = history.history\n",
    "        tr_acc.append(history_dict['accuracy'])\n",
    "        tr_loss.append(history_dict['loss'])\n",
    "        val_acc.append(history_dict['val_accuracy'])\n",
    "        val_loss.append(history_dict['val_loss'])\n",
    "    tr_acc = np.mean(tr_acc, axis=0)\n",
    "    tr_loss = np.mean(tr_loss, axis=0)\n",
    "    val_acc = np.mean(val_acc, axis=0)\n",
    "    val_loss = np.mean(val_loss, axis=0)\n",
    "    max_val_acc = np.max(val_acc)\n",
    "    if show_graph:\n",
    "        print(\"The maximum validation accuracy reached is\", max_val_acc)\n",
    "        show_nn_evolution(tr_acc, tr_loss, val_acc, val_loss)\n",
    "    if return_max_val_acc:\n",
    "        return max_val_acc\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_batch_size(x_train, y_train, epochs=50, layers_size=[30,30], batch_normalization=True,\n",
    "                        dropout=None, verbose=0):\n",
    "    history_max_val_acc = []\n",
    "    batch_size_try = list(range(1,4,1))\n",
    "    for batch_size in batch_size_try:\n",
    "        print(\"Processing batch size\", batch_size)\n",
    "        if batch_size<10:\n",
    "            epochs = 35\n",
    "        max_val_acc = evaluate_nn(x_train_nn, y_train_nn, epochs=epochs,\n",
    "                                layers_size=layers_size,\n",
    "                                batch_normalization=batch_normalization,\n",
    "                                dropout=dropout,\n",
    "                                batch_size=batch_size,\n",
    "                                verbose=0,\n",
    "                                return_max_val_acc=True,\n",
    "                                show_graph=False)\n",
    "        history_max_val_acc.append(max_val_acc)\n",
    "    plt.plot(batch_size_try, history_max_val_acc)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pars(layer_size, batch_norm, dropout):\n",
    "    print('Evaluating nn configuration:\\nLayer size:\\t', layer_size, '\\nBatch normalization:\\t', batch_norm,\n",
    "         '\\nDropout:\\t', dropout)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_nn_parameters(parameters):\n",
    "    fig = plt.figure(figsize=(25,25))\n",
    "    indx = 0\n",
    "    for parameter in parameters:\n",
    "        layers_size = [25,25]\n",
    "        batch_normalization = True\n",
    "        dropout = 0.45\n",
    "        result = []\n",
    "        epochs = 130\n",
    "        batch_size = 80\n",
    "        if parameter=='layers_size':\n",
    "            layers_size_try = parameters[parameter]\n",
    "            for layers_size in layers_size_try:\n",
    "                print_pars(layers_size, batch_normalization, dropout)\n",
    "                max_val_acc = evaluate_nn(x_train_nn, y_train_nn, epochs=epochs,\n",
    "                                layers_size=layers_size,\n",
    "                                batch_normalization=batch_normalization,\n",
    "                                dropout=dropout,\n",
    "                                batch_size=batch_size,\n",
    "                                verbose=0,\n",
    "                                return_max_val_acc=True,\n",
    "                                show_graph=False)\n",
    "                result.append(max_val_acc)\n",
    "            indx+=1\n",
    "            ax = plt.subplot(math.ceil(len(parameters)/2),2,indx)\n",
    "            ax.set_title(parameter)\n",
    "            ax.plot(range(len(result)), result)\n",
    "            \n",
    "        if parameter=='batch_normalization':\n",
    "            batch_normalization_try = parameters[parameter]\n",
    "            for batch_normalization in batch_normalization_try:\n",
    "                print_pars(layers_size, batch_normalization, dropout)\n",
    "                max_val_acc = evaluate_nn(x_train_nn, y_train_nn, epochs=epochs,\n",
    "                                layers_size=layers_size,\n",
    "                                batch_normalization=batch_normalization,\n",
    "                                dropout=dropout,\n",
    "                                batch_size=batch_size,\n",
    "                                verbose=0,\n",
    "                                return_max_val_acc=True,\n",
    "                                show_graph=False)\n",
    "                result.append(max_val_acc)\n",
    "            indx+=1\n",
    "            ax = plt.subplot(math.ceil(len(parameters)/2),2,indx)\n",
    "            ax.set_title(parameter)\n",
    "            ax.plot(range(len(result)), result)\n",
    "            \n",
    "        if parameter=='dropout':\n",
    "            dropout_try = parameters[parameter]\n",
    "            for dropout in dropout_try:\n",
    "                print_pars(layers_size, batch_normalization, dropout)\n",
    "                max_val_acc = evaluate_nn(x_train_nn, y_train_nn, epochs=epochs,\n",
    "                                layers_size=layers_size,\n",
    "                                batch_normalization=batch_normalization,\n",
    "                                dropout=dropout,\n",
    "                                batch_size=batch_size,\n",
    "                                verbose=0,\n",
    "                                return_max_val_acc=True,\n",
    "                                show_graph=False)\n",
    "                result.append(max_val_acc)\n",
    "            indx+=1\n",
    "            ax = plt.subplot(math.ceil(len(parameters)/2),2,indx)\n",
    "            ax.set_title(parameter)\n",
    "            ax.plot(dropout_try, result)\n",
    "    \n",
    "    return\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network - Create and train final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_nn_models(dropouts, batch_normalizations, layers_sizes, input_shape,\n",
    "                      best_dropout, best_batch_normalization, best_layers_sizes, with_weights=True):\n",
    "    models = []\n",
    "    weights = []\n",
    "    for dropout in dropouts:\n",
    "        for batch_normalization in batch_normalizations:\n",
    "            for layers_size in layers_sizes:\n",
    "                model = build_model(input_shape=input_shape,\n",
    "                                    layers_size=layers_size,\n",
    "                                    batch_normalization=batch_normalization,\n",
    "                                    dropout=dropout)\n",
    "                models.append(model)\n",
    "                if (dropout==best_dropout) and (batch_normalization==best_batch_normalization):\n",
    "                    if layers_size in best_layers_sizes:\n",
    "                        weights.append(2)\n",
    "                    else:\n",
    "                        weights.append(1)\n",
    "                else:\n",
    "                    weights.append(1)\n",
    "    if with_weights:\n",
    "        return models, weights\n",
    "    else:\n",
    "        return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_fit(data_train, counter):\n",
    "    data_sig = data_train[0]\n",
    "    data_bkg = data_train[1].sample(frac=1).reset_index(drop=True)[:len(data_sig)]\n",
    "    data_tot = data_sig.append(data_bkg, ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    X_train = data_tot.iloc[:,:-1]\n",
    "    Y_train = data_tot.iloc[:,-1]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train, Y_train)\n",
    "    scaler_path = os.path.join('Scalers', 'std_scaler_' + str(counter) + '.bin')\n",
    "    joblib.dump(scaler, scaler_path, compress=True)\n",
    "    return X_train, Y_train, scaler, scaler_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_data_for_fit(data_train):\n",
    "    data_sig = data_train[0].sample(frac=1).reset_index(drop=True)\n",
    "    data_bkg = data_train[1].sample(frac=1).reset_index(drop=True)\n",
    "    data_sig_val = data_sig[:int(0.3*len(data_sig))]\n",
    "    data_bkg_val = data_bkg[:len(data_sig_val)]\n",
    "    data_val = data_sig_val.append(data_bkg_val, ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    X_val = data_val.iloc[:,:-1]\n",
    "    Y_val = data_val.iloc[:,-1]\n",
    "    data_remaining = [data_sig[int(0.3*len(data_sig)):], data_bkg[len(data_sig_val):]]\n",
    "    return data_remaining, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_nn(x_train, y_train, x_test, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    y_train = np.asarray(y_train, dtype='float32')\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_test = np.asarray(y_test, dtype='float32')\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_nn_ensemble(x_train, y_train, x_val, y_val, scaler):\n",
    "    x_train = scaler.transform(x_train)\n",
    "    y_train = np.asarray(y_train, dtype='float32')\n",
    "    x_val = scaler.transform(x_val)\n",
    "    y_val = np.asarray(y_val, dtype='float32')\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nn_models(models, data_train, batch_size, epochs, just_best_models=True):\n",
    "    \n",
    "    counter = 1\n",
    "    ckpt_paths = []\n",
    "    scalers_paths = []\n",
    "    \n",
    "    if just_best_models:\n",
    "        start_file_name = 'best_model_'\n",
    "    else:\n",
    "        start_file_name = 'model_'\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        data_training, X_val, Y_val = get_val_data_for_fit(data_train)\n",
    "        print('Processing', counter, '/', len(models), 'model.')\n",
    "        \n",
    "        X_train, Y_train, scaler, scaler_path = get_data_for_fit(data_training, counter)\n",
    "        scalers_paths.append(scaler_path)\n",
    "        \n",
    "        X_train, Y_train, X_val, Y_val = prepare_data_nn_ensemble(X_train, Y_train, X_val, Y_val, scaler)\n",
    "        \n",
    "        ckpt_name = start_file_name + str(counter) + '.ckpt'\n",
    "        ckpt_path = os.path.join('Models_checkpoints', ckpt_name)\n",
    "        checkpoint = ModelCheckpoint(ckpt_path, monitor='val_accuracy', verbose=1,\n",
    "                                     save_best_only=True, mode='max')\n",
    "        callbacks_list = [checkpoint]\n",
    "        model.fit(X_train, Y_train,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  callbacks=callbacks_list,\n",
    "                  validation_data=(X_val, Y_val),\n",
    "                  verbose=0\n",
    "                 )\n",
    "        \n",
    "        ckpt_paths.append(ckpt_path)\n",
    "        counter += 1\n",
    "    return models, ckpt_paths, scalers_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_checkpoints_to_file(checkpoints, all_models=True):\n",
    "    if all_models:\n",
    "        file_path = 'all_models_checkpoints.txt'\n",
    "    else:\n",
    "        file_path = 'best_models_checkpoints.txt'\n",
    "    with open(file_path, 'w+') as f:\n",
    "        for checkpoint in checkpoints:\n",
    "            f.write(checkpoint + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_scalers_to_file(scalers, all_models=True):\n",
    "    if all_models:\n",
    "        file_path = 'all_models_scalers.txt'\n",
    "    else:\n",
    "        file_path = 'best_models_scalers.txt'\n",
    "    with open(file_path, 'w+') as f:\n",
    "        for scaler in scalers:\n",
    "            f.write(scaler + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network - Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_test_data_ensemble_nn():\n",
    "    x_train = pd.read_csv('Data_test/x_test_nn_ensemble.csv', index_col=0, sep=\",\")\n",
    "    y_train = pd.read_csv('Data_test/y_test_nn_ensemble.csv', index_col=0, sep=\",\", header=None, names=['Label'])\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_models(models, all_models=True):\n",
    "    if all_models:\n",
    "        checkpoints_path = 'all_models_checkpoints.txt'\n",
    "        scalers_path = 'all_models_scalers.txt'\n",
    "    else:\n",
    "        checkpoints_path = 'best_models_checkpoints.txt'\n",
    "        scalers_path = 'best_models_scalers.txt'\n",
    "        \n",
    "    with open(checkpoints_path, 'r') as f:\n",
    "        checkpoints = f.readlines()\n",
    "    checkpoints = [checkpoint[:-1] for checkpoint in checkpoints]\n",
    "    models_loaded = []\n",
    "    for model, checkpoint in zip(models, checkpoints):\n",
    "        model.load_weights(checkpoint)\n",
    "        models_loaded.append(model)\n",
    "        \n",
    "    with open(scalers_path, 'r') as f:\n",
    "        scalers = f.readlines()\n",
    "    scalers = [scaler[:-1] for scaler in scalers]\n",
    "    scalers_loaded = []\n",
    "    for scaler in scalers:\n",
    "        scaler_loaded = joblib.load(scaler)\n",
    "        scalers_loaded.append(scaler_loaded)\n",
    "        \n",
    "    return models_loaded, scalers_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble_NN():\n",
    "    \n",
    "    def __init__(self, models, scalers, weights=None):\n",
    "        self.models = models\n",
    "        self.scalers = scalers\n",
    "        self.weights = weights\n",
    "        \n",
    "    def predict(self, x_train):\n",
    "        votes_proba = np.zeros((x_train.shape[0],1))\n",
    "        for model, scaler in zip(self.models, self.scalers):\n",
    "            x_train_model = scaler.transform(x_train)\n",
    "            vote = model.predict(x_train_model)\n",
    "            votes_proba += np.array(vote)\n",
    "        votes_proba /= len(self.models)\n",
    "        return np.round(votes_proba).astype(int)\n",
    "    \n",
    "    def predict_proba(self, x_train):\n",
    "        votes_proba = np.zeros((x_train.shape[0],1))\n",
    "        for model, scaler in zip(self.models, self.scalers):\n",
    "            x_train_model = scaler.transform(x_train)\n",
    "            vote = model.predict(x_train_model)\n",
    "            votes_proba += np.array(vote)\n",
    "        votes_proba /= len(self.models)\n",
    "        return votes_proba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
