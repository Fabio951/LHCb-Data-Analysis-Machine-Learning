{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and manage datas - Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    data_folder = 'Data_updated'\n",
    "    if file==\"simulation_bkg\":\n",
    "        file_name = os.path.join(data_folder,\"background.txt\")\n",
    "    elif file==\"simulation_signal\":\n",
    "        file_name = os.path.join(data_folder,\"MC_signal.txt\")\n",
    "    elif file==\"data\":\n",
    "        file_name = os.path.join(data_folder,\"data_lhcb.txt\")\n",
    "    else:\n",
    "        raise Exception(\"file should be 'simulation_bkg', 'simulation_signal' or 'data'.\")\n",
    "        return \n",
    "    return pd.read_csv(file_name, index_col=0, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_none(data, file):\n",
    "    check = data.isnull().any()\n",
    "    if np.sum(check)==0:\n",
    "        return\n",
    "    else:\n",
    "        print('File', file, 'has some None values.')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_datas(labels=True, check_none=True):\n",
    "    files = ['simulation_bkg', 'simulation_signal', 'data']\n",
    "    datas = []\n",
    "    if labels==False:\n",
    "        for file in files:\n",
    "            datas.append(get_data(file))\n",
    "    else:\n",
    "        for file in files:\n",
    "            data = get_data(file)\n",
    "            check_for_none(data, file)\n",
    "            if file=='simulation_bkg':\n",
    "                data['Label'] = 0\n",
    "            if file=='simulation_signal':\n",
    "                data['Label'] = 1\n",
    "            datas.append(data)\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complementary_cut(data):\n",
    "    cut_1 = data[\"DeltaM_F\"]<360\n",
    "    cut_2 = data[\"Lambda_b0_MM_F\"]>5550 \n",
    "    cut_3 = data[\"Lambda_b0_MM_F\"]<5680\n",
    "    data = data[cut_1 | (cut_2 & cut_3)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Manage datas - Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_datas(data, only_useful=False, inplace=False, only_unbiased=False):\n",
    "    meaningless = [\"Lambda_b0_BKGCAT_F\",\"lcstar_BKGCAT_F\",\"Lambda_c_BKGCAT_F\"]\n",
    "    biased = [\"lcstar_MM_F\", \"Lambda_b0_MM_F\", \"DeltaM_F\", \"pair_lcstar_F\"]\n",
    "    useless = [\"tau_pion0_ProbNNpi_F\", \"tau_pion1_ProbNNpi_F\", \"tau_pion2_ProbNNpi_F\", \n",
    "                \"lcstar_pim_ProbNNpi_F\", \"lcstar_pip_ProbNNpi_F\", \"Lambda_b0_ENDVERTEX_CHI2_F\", \n",
    "                \"Lambda_c_ENDVERTEX_CHI2_F\", \"lcstar_ENDVERTEX_CHI2_F\"]\n",
    "    drop = meaningless\n",
    "    if only_useful:\n",
    "        drop += useless\n",
    "    if only_unbiased:\n",
    "        drop += biased\n",
    "    col_left = set(data.columns)\n",
    "    drop = col_left & set(drop)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        if inplace:\n",
    "            data.drop(columns=drop, inplace=True)\n",
    "            return\n",
    "        else:\n",
    "            return data.drop(columns=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_useful_features(datas):\n",
    "    return_datas = []\n",
    "    for data in datas:\n",
    "        return_datas.append(clean_datas(data, only_useful=True, inplace=False))\n",
    "    return return_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class correlation_cleaner():\n",
    "    \n",
    "    def __init__(self, keep=None, above=0.9):\n",
    "        self.keep = keep\n",
    "        self.above = above\n",
    "        self.discarded = set()\n",
    "        \n",
    "    def fit(self, data):\n",
    "        corr_mat = data.corr()\n",
    "        corr_mat[corr_mat.isnull()] = 0\n",
    "        corr_mat = corr_mat.values\n",
    "        col_names = data.columns\n",
    "        for i in range(corr_mat.shape[1]):\n",
    "            corr_mat[i,i] = 0\n",
    "        col_discard = set()\n",
    "        if self.keep!=None:\n",
    "            max_val = np.max(np.abs(corr_mat))\n",
    "            for i in range(data.shape[1]):\n",
    "                for j in range(i+1, data.shape[1]):\n",
    "                    if np.abs(corr_mat[i,j])>0.9*max_val:\n",
    "                        col_discard.add(col_names[i])\n",
    "                        break\n",
    "        else:\n",
    "            for i in range(data.shape[1]):\n",
    "                for j in range(i+1, data.shape[1]):\n",
    "                    if np.abs(corr_mat[i,j])>self.above:\n",
    "                        col_discard.add(col_names[i])\n",
    "                        break\n",
    "        self.discard = col_discard\n",
    "        self.discarded = self.discarded.union(col_discard)\n",
    "        #print('Column(s) about to be discarded:', self.discard)\n",
    "        \n",
    "    def transform(self, data, inplace=False):\n",
    "        if inplace==False:\n",
    "            return data.drop(columns=self.discarded)\n",
    "        else:\n",
    "            data.drop(columns=self.discarded, inplace=True)\n",
    "        \n",
    "    def fit_transform(self, data, inplace_=False, show_corr_mat=False):\n",
    "        self.fit(data)\n",
    "        if show_corr_mat:\n",
    "            show_corr(data, small=True)\n",
    "        if inplace_==False:\n",
    "            return data.drop(columns=self.discard)\n",
    "        else:\n",
    "            data.drop(columns=self.discard, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncorrelated_features(datas, cleaner):\n",
    "    return_datas = []\n",
    "    for data in datas:\n",
    "        return_datas.append(cleaner.transform(data, inplace=False))\n",
    "    return return_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_test_set(datas_or, write_on_file=True):\n",
    "    data_sim_sign = datas_or[1].sample(frac=1).reset_index(drop=True)\n",
    "    data_bkg = datas_or[0].sample(frac=1).reset_index(drop=True)\n",
    "    train_size = 0.7\n",
    "    data_sig_train = data_sim_sign[:int(train_size*len(data_sim_sign))]\n",
    "    data_bkg_train = data_bkg[:int(train_size*len(data_bkg))]\n",
    "    data_train = [data_bkg_train, data_sig_train]\n",
    "    data_sig_test = data_sim_sign[int(train_size*len(data_sim_sign)):]\n",
    "    data_bkg_test = data_bkg[int(train_size*len(data_bkg)):]\n",
    "    data_test = data_sig_test.append(data_bkg_test[:len(data_sig_test)],\n",
    "                                     ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    x_test = data_test.iloc[:,:-1]\n",
    "    y_test = data_test.iloc[:,-1]\n",
    "    if write_on_file:\n",
    "        x_test.to_csv('Data_test/x_test.csv', mode='w+')\n",
    "        y_test.to_csv('Data_test/y_test.csv', mode='w+')\n",
    "    return data_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_datas(datas, density=True, cuts=False, choose_logs=False):\n",
    "    col_names = datas[0].columns.tolist()\n",
    "    if \"Label\" in col_names:\n",
    "        col_names.remove(\"Label\")\n",
    "\n",
    "    fig = plt.figure(figsize=(25,100))\n",
    "    ranges = [None]*len(col_names)\n",
    "    if cuts:\n",
    "        ranges = [None, None, [3000,8000],None, [-15,25], None,\n",
    "                 [-1.5,1.5], [-2,2], [0,10], None, None, None,\n",
    "                 None, None, None, None, None, None,\n",
    "                 None, None, None, None, None, None, None]\n",
    "    logs = [False]*len(col_names)\n",
    "    if choose_logs:\n",
    "        logs = [True, True, True, True, False, False,\n",
    "              False, False, False, False, False, False,\n",
    "              True, True, False, True, True, True,\n",
    "              True, True, True, True, False, False, False]\n",
    "    colors = [\"red\", \"green\", \"blue\"]\n",
    "    ind=0\n",
    "    for col, range_, log in zip(col_names, ranges, logs):\n",
    "        ind+=1\n",
    "        ax = plt.subplot(math.ceil(len(col_names)/2),2,ind)\n",
    "        ax.set_title(col)\n",
    "        for data, color in zip(datas, colors):\n",
    "            ax.hist(data[col], color=color, density=density, alpha=0.5, bins=60, log=log, range=range_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_corr(data, small=False, norm=False, delete_variance=True):\n",
    "    dim=5\n",
    "    if small:\n",
    "        dim = 3\n",
    "    fig = plt.figure(figsize=(dim,dim))\n",
    "    corr_mat = data.corr()\n",
    "    corr_mat[corr_mat.isnull()] = 0\n",
    "    corr_mat = corr_mat.values\n",
    "    if delete_variance:\n",
    "        for i in range(data.shape[1]):\n",
    "            corr_mat[i,i] = 0\n",
    "    if norm==False:\n",
    "        plt.imshow(corr_mat, cmap='bwr')\n",
    "    else:\n",
    "        plt.imshow(corr_mat, vmin=-1, vmax=1, cmap='bwr')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common functions both algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_datas(datas, max_samples):\n",
    "    data_sig = datas[1]\n",
    "    data_bkg = datas[0].sample(frac=1).reset_index(drop=True)[:max_samples]\n",
    "    data_train = data_sig.append(data_bkg, ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "    return data_train.iloc[:,:-1], data_train.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_mod(datas, max_samples=1e10):\n",
    "    data_sig = datas[1]\n",
    "    data_bkg = datas[0]\n",
    "    if data_sig.shape[0]>3000:\n",
    "        data_sig = data_sig.sample(frac=1).reset_index(drop=True)[:max_samples]\n",
    "    if data_bkg.shape[0]>3000:\n",
    "        data_bkg = data_bkg.sample(frac=1).reset_index(drop=True)[:max_samples]\n",
    "    data_tot = data_sig.append(data_bkg, ignore_index=True)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_tot.iloc[:,:-1], data_tot.iloc[:,-1],\n",
    "                                                        test_size=0.7, shuffle=True,\n",
    "                                                        stratify=data_tot.iloc[:,-1])\n",
    "    x_train.index = np.arange(1,len(x_train)+1)\n",
    "    y_train.index = np.arange(1,len(y_train)+1)\n",
    "    x_test.index = np.arange(1,len(x_test)+1)\n",
    "    y_test.index = np.arange(1,len(y_test)+1)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(x_train, x_test, optimized=False, useful=False):\n",
    "    x_train_usf, x_test_usf = get_useful_features([x_train, x_test])\n",
    "    if useful:\n",
    "        return x_train_usf, x_test_usf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(datas, useful=False, optimized=False,\n",
    "                 return_all_datas_optimized=False, return_all_datas_useful=False):\n",
    "    data_bkg_usf, data_sim_sign_usf, data_lhcb_usf, data_compl_usf = get_useful_features(datas)\n",
    "    datas_usf = [data_bkg_usf, data_sim_sign_usf, data_lhcb_usf, data_compl_usf]\n",
    "    \n",
    "    if return_all_datas_useful:\n",
    "        return datas_usf\n",
    "    if useful:\n",
    "        return train_test_split_mod([data_bkg_usf, data_sim_sign_usf], max_samples=1300)\n",
    "    \n",
    "    cleaner = correlation_cleaner(above=0.8)\n",
    "    cleaner.fit(data_lhcb_usf)\n",
    "    \n",
    "    data_bkg_opt, data_sim_sign_opt, data_lhcb_opt, data_compl_opt = get_uncorrelated_features(datas_usf, cleaner)\n",
    "    datas_optimized = [data_bkg_opt, data_sim_sign_opt, data_lhcb_opt, data_compl_opt]\n",
    "    \n",
    "    if return_all_datas_optimized:\n",
    "        return datas_optimized\n",
    "    \n",
    "    return train_test_split_mod([data_bkg_opt, data_sim_sign_opt], max_samples=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(y_pred_prob, y_test, title=None, small=False, nn=False):\n",
    "    if nn:\n",
    "        y_pred_prob_hist = pd.DataFrame(y_pred_prob, columns=[\"Label\"])\n",
    "    else:\n",
    "        y_pred_prob_hist = pd.DataFrame([prob[0] for prob in y_pred_prob], columns=[\"Label\"])\n",
    "    y_test_hist = pd.DataFrame(y_test.values, columns=[\"Label\"])\n",
    "    size = (5,5)\n",
    "    if small:\n",
    "        size = (3,3)\n",
    "    fig = plt.figure(figsize=size)\n",
    "    plt.hist(y_pred_prob_hist[(y_test_hist==1).values][\"Label\"], density=True, alpha=0.5, log=False)\n",
    "    plt.hist(y_pred_prob_hist[(y_test_hist==0).values][\"Label\"], density=True, alpha=0.5, log=False)\n",
    "    plt.title(title)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_result(clf, x_test, y_test, nn=False):\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(y_pred.shape)\n",
    "    print(y_test.shape)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('Accuracy: \\t', accuracy )\n",
    "    y_pred_prob = clf.predict_proba(x_test)\n",
    "    show_result(y_pred_prob, y_test, nn)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best number of samples from the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plots_fraction_tot_samples(data1, data2, samples):\n",
    "    best_forest_clf = RandomForestClassifier(n_estimators=300, max_features=3, bootstrap=True, n_jobs=-1)\n",
    "    fig = plt.figure(figsize=(25,25))\n",
    "    ind=0\n",
    "    for sample in samples:\n",
    "        ind+=1\n",
    "        x_train_opt, y_train_opt, x_test_opt, y_test_opt = train_test_split_mod([data1, data2],\n",
    "                                                                                max_samples=sample)\n",
    "        best_forest_clf.fit(x_train_opt, y_train_opt)\n",
    "        y_pred_opt = best_forest_clf.predict_proba(x_test_opt)\n",
    "        show_result(y_pred_opt, y_test_opt, title=str(sample) + \" samples of the background\", small=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
